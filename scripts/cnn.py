# -*- coding: utf-8 -*-
"""Word Recognition Using Modified Samples.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1E1WTWG5LgPXD1eQXcIeR2tX9F08Oppoc
"""

# Commented out IPython magic to ensure Python compatibility.
import os
import cv2
import pandas as pd
import numpy as np
import random
import scipy
from scipy.io import wavfile
import scipy.fftpack as fft
from scipy.signal import get_window
import IPython.display as ipd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from keras import backend as K
from keras.models import Model, Sequential
from keras.layers import Dropout, Flatten, Dense, Activation, Conv2D, MaxPooling2D
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.utils import to_categorical
from tqdm import tqdm

def val_acc(accent,snr,noise,feature):

    X_train=[]
    Z_train=[]

    IMG_SIZE=256

    DIR_train = os.path.join('E:\\ACCENTS - Copie\\', accent, 'noise_modified_samples', snr, noise, feature, 'french_training\\')

    BAGS_DIR_train = DIR_train + 'bags'
    KIDS_DIR_train = DIR_train + 'kids'
    STATION_DIR_train = DIR_train + 'station'
    PLEASE_DIR_train = DIR_train + 'please'
    STORE_DIR_train = DIR_train + 'store'

    X_test=[]
    Z_test=[]

    IMG_SIZE=256

    DIR_test = os.path.join('E:\\ACCENTS - Copie\\', accent, 'noise_modified_samples', snr, noise, feature, 'french_testing_modified\\')

    BAGS_DIR_test = DIR_test + 'bags'
    KIDS_DIR_test = DIR_test + 'kids'
    STATION_DIR_test = DIR_test + 'station'
    PLEASE_DIR_test = DIR_test + 'please'
    STORE_DIR_test = DIR_test + 'store'

    def target(img, word):
        return word

    def data_creation(word,X,Z,DIR):
        for img in tqdm(os.listdir(DIR)):
            name = img
            label = target(img,word)
            path = os.path.join(DIR,img)
            img = cv2.imread(path,cv2.IMREAD_COLOR)
            img = cv2.resize(img, (IMG_SIZE,IMG_SIZE))

            X.append(np.array(img))
            Z.append([str(label),name])

    # Creation of the data corresponding to the 5 words

    data_creation('bags',X_train,Z_train,BAGS_DIR_train)
    data_creation('kids',X_train,Z_train,KIDS_DIR_train)
    data_creation('station',X_train,Z_train,STATION_DIR_train)
    data_creation('please',X_train,Z_train,PLEASE_DIR_train)
    data_creation('store',X_train,Z_train,STORE_DIR_train)

    data_creation('bags',X_test,Z_test,BAGS_DIR_test)
    data_creation('kids',X_test,Z_test,KIDS_DIR_test)
    data_creation('station',X_test,Z_test,STATION_DIR_test)
    data_creation('please',X_test,Z_test,PLEASE_DIR_test)
    data_creation('store',X_test,Z_test,STORE_DIR_test)

    # We transform the data in a way understandable by a Deep-Learning model

    le_train = LabelEncoder()
    L_train = []
    for i in range(len(Z_train)):
        L_train.append(Z_train[i][0])
    Y_train = le_train.fit_transform(L_train)
    Y_train = to_categorical(Y_train,5)
    l_train = []
    for i in range(len(Y_train)):
        l_train.append([Y_train[i], Z_train[i][1]])
    Y_train = l_train
    X_train = np.array(X_train)
    X_train = X_train/255

    le_test = LabelEncoder()
    L_test = []
    for i in range(len(Z_test)):
        L_test.append(Z_test[i][0])
    Y_test = le_test.fit_transform(L_test)
    Y_test = to_categorical(Y_test,5)
    l_test= []
    for i in range(len(Y_test)):
        l_test.append([Y_test[i], Z_test[i][1]])
    Y_test = l_test
    X_test = np.array(X_test)
    X_test = X_test/255

    x_train,x_test,y_train,y_test = X_train, X_test, Y_train, Y_test

    # We build y_train and y_test (with just the labels)

    train_list = []
    for l in y_train:
        train_list.append(l[0])

    test_list = []
    for l in y_test:
        test_list.append(l[0])

    y_train = train_list
    y_test = test_list

    # We turn y_train and y_test into a single array, so we can use it in the model

    for i in range(len(y_train)):
        y_train[i] = list(y_train[i])
    y_train = np.asarray(y_train)

    for i in range(len(y_test)):
        y_test[i] = list(y_test[i])
    y_test = np.asarray(y_test)

    model = Sequential()
    model.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same',activation ='relu', input_shape = (IMG_SIZE,IMG_SIZE,3)))
    model.add(MaxPooling2D(pool_size=(2,2)))


    model.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same',activation ='relu'))
    model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))

    model.add(Conv2D(filters = 96, kernel_size = (3,3),padding = 'Same',activation ='relu'))
    model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))

    model.add(Conv2D(filters = 96, kernel_size = (3,3),padding = 'Same',activation ='relu'))
    model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))

    model.add(Flatten())
    model.add(Dense(512))
    model.add(Activation('relu'))
    model.add(Dense(5, activation = "softmax"))

    model.compile(optimizer=Adam(),loss='categorical_crossentropy',metrics=['accuracy'])
    model.summary()

    batch_size=50
    epochs=500

    history = model.fit(x_train,y_train,epochs=epochs,batch_size=batch_size,validation_data = (x_test,y_test))
    return round(history.history['val_accuracy'][-1]*100)/100

def process():

    list_accents = ['arabic','french']
    list_snr = ['noise_-5dB','noise_0dB','noise_5dB','noise_10dB','clean']
    list_noises = ['babble','hf_channel','white_noise']
    list_features = ['mfcc','mfcc+fourier','mfcc+instant','mfcc+fourier+instant','fourier+instant']

    df = pd.DataFrame(columns=['accent', 'snr', 'noise', 'feature', 'accuracy'])

    for accent in list_accents:
        for snr in list_snr:
            for noise in list_noises:
                for feature in list_features:
                    accuracy = val_acc(accent, snr, noise, feature)
                    print(accent, snr, noise, feature, accuracy)
                    new_row = {'accent': accent, 'snr': snr,  'noise': noise, 'feature': feature, 'accuracy': accuracy}
                    df = df.append(new_row, ignore_index=False)

    return df